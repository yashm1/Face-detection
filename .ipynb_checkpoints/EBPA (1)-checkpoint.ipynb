{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all import statements\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data transformation on saved input /output matrices\n",
    "X_train = np.load(\"X_basic.npy\")\n",
    "X_train /= 255.\n",
    "Y_train = np.load(\"Y_basic.npy\")\n",
    "m = X_train.shape[1] #number of training examples\n",
    "\n",
    "# X_test = None\n",
    "# Y_test = None\n",
    "\n",
    "#shape of X should be like (values in vector of single example , number of examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "n_layers = 4\n",
    "layers = [1024,512 , 512 , 256 , 256 , 100] #List containing number of  neurons in each layer\n",
    "assert(layers[0] == X_train.shape[0])  #first layer = input layer\n",
    "assert(layers[-1] == Y_train.shape[0]) #number of outputs should be equal to number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax activation will be used to get number in last layer\n",
    "#We are using ReLu activation elsewhere\n",
    "def softmax(z):\n",
    "    z = np.exp(z)\n",
    "    z = z/np.sum(z , axis = 0 , keepdims = True , dtype=np.float32) #sum along a column\n",
    "    return z\n",
    "\n",
    "def relu(z):\n",
    "    temp = z > 0\n",
    "    z = z * temp\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#derivative of softmax and relu activation\n",
    "#wrong wrong wrong\n",
    "# def d_softmax(A):\n",
    "#     retval = np.zeros(A.shape)\n",
    "#     for i in range(A.shape[0]):\n",
    "#         for j in range(A.shape[0]):\n",
    "#             if i != j:\n",
    "#                 retval[i] += -A[i]*A[j]\n",
    "#             else:\n",
    "#                 retval[i] += A[i] - A[i]**2\n",
    "#     return retval\n",
    "\n",
    "def d_relu(A):\n",
    "    return A >= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining network variables\n",
    "a = {}  #activations\n",
    "w = {}  #Weights\n",
    "b = {}  #bias\n",
    "# del_a = None   #d(error)/d(activation)\n",
    "# del_w = None   #d(error)/d(weight)\n",
    "# del_b = None   #d(error)/d(bias)\n",
    "a[0] = X_train #input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_nn(layers):\n",
    "    for i in range(1 , len(layers)):\n",
    "        w[i] = np.random.randn(layers[i] , layers[i - 1])/100. \n",
    "        b[i] = np.zeros((layers[i] , 1))\n",
    "        a[i] = relu(np.dot(w[i] , a[i - 1]) + b[i]) #dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trains neural network for single epoch\n",
    "#It implements Batch Gradient Descent instead of Stochastic Gradient Descent\n",
    "def train_one_epoch(alpha):\n",
    "    #Forward propagation:\n",
    "    \n",
    "    #using relu for all layers except last\n",
    "    for i  in range(1 , len(layers) - 1):\n",
    "        a[i] = relu(np.dot(w[i] , a[i - 1]) + b[i])\n",
    "        \n",
    "    last_index = len(layers) - 1\n",
    "    \n",
    "    #using softmax for last layer\n",
    "    a[last_index] = softmax(np.dot(w[last_index] , a[last_index - 1]) + b[last_index])\n",
    "    output = a[last_index]\n",
    "    \n",
    "    \n",
    "    #Error Calculation:\n",
    "    #softmax crossentropy was used \n",
    "    error = -1*(Y_train*np.log(output))\n",
    "    error = 1/m*np.sum(np.sum(error , axis = 1 , keepdims = True) )\n",
    "    \n",
    "    \n",
    "    #Back propagation:\n",
    "    \n",
    "    #for last layer (with softmax activation)\n",
    "    del_a =  -1 * np.divide(Y_train , output)\n",
    "    del_z = a[last_index] - Y_train      #z represents logits, activation(z) = a\n",
    "    del_w = 1/m *np.dot(del_z , a[last_index - 1].T)\n",
    "    del_b = 1/m * np.sum(del_z , axis = 1 , keepdims=True)\n",
    "    del_a = np.dot(w[last_index].T , del_z)\n",
    "    \n",
    "    #weight updation:\n",
    "    w[last_index] -= del_w*alpha\n",
    "    b[last_index] -= del_b*alpha\n",
    "    \n",
    "    #for all layers except last\n",
    "    for i in range(last_index - 1, 0, -1):\n",
    "        del_z = del_a * d_relu(a[i])\n",
    "        del_w = 1/m *np.dot(del_z , a[i - 1].T)\n",
    "        del_b = 1/m*np.sum(del_z , axis = 1 , keepdims=True)\n",
    "        del_a = np.dot(w[i].T , del_z)\n",
    "        \n",
    "        #weight updation:\n",
    "        w[i] -= del_w*alpha\n",
    "        b[i] -= del_b*alpha\n",
    "        \n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trains NN for n_epochs epochs\n",
    "learning_rate = 1\n",
    "def train(n_epochs ):\n",
    "    for i in range(n_epochs):\n",
    "        print(\"Training epoch :\" + str(i + 1) , end=\" ...... \")\n",
    "        error = train_one_epoch(learning_rate)\n",
    "        print(\"\\tDone \\t error = \" + str(error))\n",
    "        learning_rate *=0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward propagates a case which we want to test\n",
    "def find_output(sample_case):\n",
    "    last_index = len(layers) - 1\n",
    "    a[0] = sample_case\n",
    "    for i in range(1 , len(layers) - 1):\n",
    "        a[i] = relu(np.dot(w[i] , a[i - 1]) + b[i])\n",
    "    return softmax(np.dot(w[last_index] , a[last_index - 1]) + b[last_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_nn(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d494bcb65e13>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10000\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "train(10000 , 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = np.array(Image.open('Dataset/1_0_2_20161219194543059.jpg.chip.jpg').resize((32 , 32)).convert(\"L\")).reshape(1024,1)/255.\n",
    "np.argmax((find_output(img)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
