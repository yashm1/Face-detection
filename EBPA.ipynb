{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#all import statements\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#data transformation on saved input /output matrices\n",
    "X_train = None\n",
    "Y_train = None\n",
    "m = X_train.shape[1] #number of training examples\n",
    "\n",
    "X_test = None\n",
    "Y_test = None\n",
    "\n",
    "#shape of X should be like (values in vector of single example , number of examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "n_layers = None\n",
    "layers = None #List containing number of  neurons in each layer\n",
    "assert(layers[0] == X_train.shape[0])  #first layer = input layer\n",
    "assert(layers[-1] == Y_train.shape[0]) #number of outputs should be equal to number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#softmax activation will be used to get number in last layer\n",
    "#We are using ReLu activation elsewhere\n",
    "def softmax(z):\n",
    "    z = np.exp(z)\n",
    "    z = z/np.sum(z , axis = 0 , keepdims = True) #sum along a column\n",
    "    return z\n",
    "\n",
    "def relu(z):\n",
    "    temp = z > 0\n",
    "    z = z * temp\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#derivative of softmax and relu activation\n",
    "def d_softmax(A):\n",
    "    retval = np.zeros(A.shape)\n",
    "    for i in range(A.shape[0]):\n",
    "        for j in range(A.shape[0]):\n",
    "            if i != j:\n",
    "                retval[i] += -A[i]*A[j]\n",
    "            else:\n",
    "                retval[i] += A[i] - A[i]**2\n",
    "    return retval\n",
    "\n",
    "def d_relu(A):\n",
    "    return A >= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-20b225aa36b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdel_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m   \u001b[1;31m#d(error)/d(activation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdel_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m   \u001b[1;31m#d(error)/d(weight)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "#defining network variables\n",
    "a = {}  #activations\n",
    "w = {}  #Weights\n",
    "b = {}  #bias\n",
    "del_a = None   #d(error)/d(activation)\n",
    "del_w = None   #d(error)/d(weight)\n",
    "del_b = None   #d(error)/d(bias)\n",
    "a[0] = X_train #input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def initialize_nn(layers):\n",
    "    for i in range(1 , len(layers)):\n",
    "        w[i] = np.random.randn(layers[i] , layers[i - 1])\n",
    "        b[i] = np.random.randn(layers[i] , 1)\n",
    "        a[i] = relu(np.dot(w[i] , a[i - 1]) + b[i]) #dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trains neural network for single epoch\n",
    "#It implements Batch Gradient Descent instead of Stochastic Gradient Descent\n",
    "def train_one_epoch(alpha):\n",
    "    #Forward propagation:\n",
    "    \n",
    "    #using relu for all layers except last\n",
    "    for i  in range(1 , len(layers) - 1):\n",
    "        a[i] = relu(np.dot(w[i] , a[i - 1]) + b[i])\n",
    "        \n",
    "    last_index = len(layers) - 1\n",
    "    \n",
    "    #using softmax for last layer\n",
    "    a[last_index] = softmax(np.dot(w[last_index] , a[last_index - 1]) + b[last_index])\n",
    "    output = a[last_index]\n",
    "    \n",
    "    \n",
    "    #Error Calculation:\n",
    "    #binary crossentropy was used instead of softmax crossentropy\n",
    "    #because I found experimentally that it working better\n",
    "    error = -1*(Y_train*np.log(output) + (1-Y_train)*np.log(1 - output)) #binary crosserntropy\n",
    "    error = np.sum(error , axis = 1 , keepdims = True)\n",
    "    \n",
    "    \n",
    "    #Back propagation:\n",
    "    \n",
    "    #for last layer (with softmax activation)\n",
    "    del_a = (1-Y_train)/(1-output) - Y_train/output\n",
    "    del_z = del_a * d_softmax(a[last_index])       #z represents induced fiels, activation(z) = a\n",
    "    del_w = 1/m * np.dot(del_z , a[last_index - 1].T)\n",
    "    del_b = 1/m * np.sum(del_z , axis = 1 , keepdims=True)\n",
    "    del_a = np.dot(w[last_index].T , del_z)\n",
    "    \n",
    "    #weight updation:\n",
    "    w[last_index] -= del_w*alpha\n",
    "    b[last_index] -= del_b*alpha\n",
    "    \n",
    "    #for all layers except last\n",
    "    for i in range(last_index - 1, 0, -1):\n",
    "        del_z = del_a * d_relu(a[i])\n",
    "        del_w = 1/m * np.dot(del_z , a[last_index - 1].T)\n",
    "        del_b = 1/m * np.sum(del_z , axis = 1 , keepdims=True)\n",
    "        del_a = np.dot(w[i].T , del_z)\n",
    "        \n",
    "        #weight updation:\n",
    "        w[i] -= del_w*alpha\n",
    "        b[i] -= del_b*alpha\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trains NN for n_epochs epochs\n",
    "def train(n_epochs , learning_rate = 0.01):\n",
    "    for i in range(n_epochs):\n",
    "        train_one_epoch(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward propagates a case which we want to test\n",
    "def find_output(sample_case):\n",
    "    last_index = len_layers - 1\n",
    "    a[0] = sample_case\n",
    "    for i in range(1 , len(layers) - 1):\n",
    "        a[i] = relu(np.dot(w[i] , a[i - 1]) + b[i])\n",
    "    return softmax(np.dot(w[last_index] , a[last_index - 1]) + b_last[index])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
